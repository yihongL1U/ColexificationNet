{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f39b57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/mounts/data/proj/yihong/newhome/ENTER/envs/concept-net/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append(\"..\") \n",
    "import network_builder\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from networkx.algorithms import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0714b917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading directed net from /mounts/data/proj/yihong/newhome/ConceptNetwork/network_related/stored_networks/directed_network_updated.pickle ...\n"
     ]
    }
   ],
   "source": [
    "# we first build a DIRECTED graph\n",
    "# which is the base of other UNDIRECTED graphs to be constructed\n",
    "considered_lang = 'all'\n",
    "# load from disk\n",
    "net = network_builder.ConceptNetwork(involved_lang=considered_lang, \n",
    "                                     load_directed_graph_from_path=True, use_updated=True,\n",
    "                                     load_directed_graph_path= \\\n",
    "                                     '/mounts/data/proj/yihong/newhome/ConceptNetwork/network_related/stored_networks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb8b19f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 5870\n",
      "Number of edges: 1161424\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of nodes: {net.concept_net.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {net.concept_net.number_of_edges()}\")\n",
    "\n",
    "# Number of nodes: 5870\n",
    "# Number of edges: 1161424"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18fb62c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 2591\n",
      "Number of edges: 13607\n"
     ]
    }
   ],
   "source": [
    "# the following is the test\n",
    "undirected_net = net.to_undirected(aggregate_type='union', minimum_number_of_langs=50)\n",
    "print(f\"Number of nodes: {undirected_net.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {undirected_net.number_of_edges()}\")\n",
    "\n",
    "# Number of nodes: 2591\n",
    "# Number of edges: 13607"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8e11ec",
   "metadata": {},
   "source": [
    "## create subgraphs for each language family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27809efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5\n",
    "\n",
    "with open('../iso2family.pickle', 'rb') as handle:\n",
    "    iso2family = pickle5.load(handle)\n",
    "\n",
    "families = {}\n",
    "\n",
    "for key, item in iso2family.items():\n",
    "    if item[0] not in families:\n",
    "        families[item[0]] = [key]\n",
    "    else:\n",
    "        families[item[0]].append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03f0a6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sino1245\n",
      "Number of languages: 69\n",
      "Number of nodes: 2591\n",
      "Number of edges: 7460\n",
      "\n",
      "otom1299\n",
      "Number of languages: 76\n",
      "Number of nodes: 2591\n",
      "Number of edges: 7290\n",
      "\n",
      "nucl1709\n",
      "Number of languages: 92\n",
      "Number of nodes: 2591\n",
      "Number of edges: 6487\n",
      "\n",
      "indo1319\n",
      "Number of languages: 97\n",
      "Number of nodes: 2591\n",
      "Number of edges: 6546\n",
      "\n",
      "atla1278\n",
      "Number of languages: 226\n",
      "Number of nodes: 2591\n",
      "Number of edges: 7914\n",
      "\n",
      "aust1307\n",
      "Number of languages: 230\n",
      "Number of nodes: 2591\n",
      "Number of edges: 8149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subgraph_dict_language_families = {}\n",
    "for code in ['sino1245', 'otom1299', 'nucl1709', 'indo1319', 'atla1278', 'aust1307']:\n",
    "    print(code)\n",
    "    # the threshold 0.1 here controls the minimum number of languages for an edge\n",
    "    # in the original undirected_net to be added in the language family subgraph\n",
    "    # the value should be between 0 to 1\n",
    "    subgraph_dict_language_families[code] = net.create_subgraph_of_a_language_family(code, 0.1, \n",
    "                                                                                     family_path='../iso2family.pickle')\n",
    "    print(f\"Number of languages: {len(families[code])}\")\n",
    "    print(f\"Number of nodes: {subgraph_dict_language_families[code].number_of_nodes()}\")\n",
    "    print(f\"Number of edges: {subgraph_dict_language_families[code].number_of_edges()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940f1f7f",
   "metadata": {},
   "source": [
    "## create subgraphs for each area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28058234",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../iso2area.pickle', 'rb') as handle:\n",
    "    iso2area = pickle5.load(handle)\n",
    "\n",
    "areas = {}\n",
    "\n",
    "for key, item in iso2area.items():\n",
    "    if item not in areas:\n",
    "        areas[item] = [key]\n",
    "    else:\n",
    "        areas[item].append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66fe1547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "South America\n",
      "Number of languages: 170\n",
      "Number of nodes: 2591\n",
      "Number of edges: 12060\n",
      "\n",
      "North America\n",
      "Number of languages: 185\n",
      "Number of nodes: 2591\n",
      "Number of edges: 12682\n",
      "\n",
      "Eurasia\n",
      "Number of languages: 216\n",
      "Number of nodes: 2591\n",
      "Number of edges: 12102\n",
      "\n",
      "Africa\n",
      "Number of languages: 352\n",
      "Number of nodes: 2591\n",
      "Number of edges: 13452\n",
      "\n",
      "Papunesia\n",
      "Number of languages: 396\n",
      "Number of nodes: 2591\n",
      "Number of edges: 13543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subgraph_dict_areas = {}\n",
    "for code in ['South America', 'North America', 'Eurasia', 'Africa', 'Papunesia']:\n",
    "    print(code)\n",
    "    # the threshold 0.1 here controls the minimum number of languages for an edge\n",
    "    # in the original undirected_net to be added in the language family subgraph\n",
    "    # the value should be between 0 to 1\n",
    "    subgraph_dict_areas[code] = net.create_subgraph_of_an_area(code, 0.1, area_path='../iso2area.pickle')\n",
    "    print(f\"Number of languages: {len(areas[code])}\")\n",
    "    print(f\"Number of nodes: {subgraph_dict_areas[code].number_of_nodes()}\")\n",
    "    print(f\"Number of edges: {subgraph_dict_areas[code].number_of_edges()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0899d0d1",
   "metadata": {},
   "source": [
    "## Compare similarity of language family specific networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c217759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "import community as community_louvain\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6605efa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each language family, we fix the oder of the nodes (stored in a list),\n",
    "# and create another list where each number indicates the community\n",
    "# which the correspdonding concept is in\n",
    "nodes_list = list(undirected_net.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1b93a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the base ConceptNet to the dictionary\n",
    "subgraph_dict_language_families['base'] = undirected_net\n",
    "language_families = list(subgraph_dict_language_families.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf01d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on 50 random community partition, we compute the mean and variance of \n",
    "# pairwise ARI among 6 + 1 (base) language families\n",
    "\n",
    "pairwise_ARI_families = {}\n",
    "\n",
    "for i in range(len(language_families)):\n",
    "    lf1 = language_families[i]\n",
    "    net1 = subgraph_dict_language_families[lf1]\n",
    "    for j in range(i, len(language_families)):\n",
    "        lf2 = language_families[j]\n",
    "        net2 = subgraph_dict_language_families[lf2]\n",
    "        # we randomly do 50 partitions for each language family\n",
    "        for i in range(50):\n",
    "            # make sure the random seed is the same when computing ARI between two families each time\n",
    "            seed = random.randint(0,114514)\n",
    "\n",
    "            # community partion of net1\n",
    "            partition_dict = community_louvain.best_partition(net1, resolution=0.1, random_state=seed)\n",
    "            partion_1 = [partition_dict[node] for node in nodes_list]\n",
    "\n",
    "            # community partion of net2\n",
    "            partition_dict = community_louvain.best_partition(net2, resolution=0.1, random_state=seed)\n",
    "            partion_2 = [partition_dict[node] for node in nodes_list]\n",
    "\n",
    "            if f\"{lf1}-{lf2}\" not in pairwise_ARI_families:\n",
    "                pairwise_ARI_families[f\"{lf1}-{lf2}\"] = [round(adjusted_rand_score(partion_1, partion_2), 2)]\n",
    "            else:\n",
    "                pairwise_ARI_families[f\"{lf1}-{lf2}\"].append(round(adjusted_rand_score(partion_1, partion_2), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b00536",
   "metadata": {},
   "source": [
    "### Adjusted rand index\n",
    "\n",
    "We will use adjusted rand index (ARI) to measure the similarity of the community structure between different language families. Generally speaking, the larger the ARI score, the more similar the family-specific networks are. ARI is symmetric, i.e., ARI(P1, P2) = ARI(P2, P1). But note that there will be some \"isolated\" concepts in these sub-networks, which can affect the ARI (making them artificially low in general).\n",
    "\n",
    "***Negative ARI values*** indicate that two networks’ community partitions vary more than would be expected by chance,  \n",
    "***ARI values of 0*** indicate that two networks’ community partitions vary at a level that would be expected at chance,   \n",
    "and ***ARI values approaching 1*** reflect high agreement in community structure between two networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c7407c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tsino\t\totom\t\tnucl\t\tindo\t\tatla\t\taust\t\tbase\t\t\n",
      "sino\t1.0(0.0)\t0.37(0.01)\t0.35(0.01)\t0.42(0.09)\t0.47(0.03)\t0.44(0.01)\t0.33(0.02)\t\n",
      "otom\t\t\t1.0(0.0)\t0.4(0.01)\t0.29(0.06)\t0.41(0.01)\t0.43(0.02)\t0.3(0.02)\t\n",
      "nucl\t\t\t\t\t1.0(0.0)\t0.27(0.06)\t0.38(0.01)\t0.43(0.01)\t0.27(0.02)\t\n",
      "indo\t\t\t\t\t\t\t1.0(0.0)\t0.37(0.08)\t0.34(0.06)\t0.29(0.02)\t\n",
      "atla\t\t\t\t\t\t\t\t\t1.0(0.0)\t0.49(0.02)\t0.35(0.02)\t\n",
      "aust\t\t\t\t\t\t\t\t\t\t\t1.0(0.0)\t0.35(0.02)\t\n",
      "base\t\t\t\t\t\t\t\t\t\t\t\t\t1.0(0.0)\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ptr_string = [f\"\\t\"] + [f\"{lf[:4]}\\t\\t\" for lf, pl in subgraph_dict_language_families.items()]\n",
    "ptr_string = ''.join(ptr_string) + '\\n'\n",
    "for i in range(len(language_families)):\n",
    "    lf1 = language_families[i]\n",
    "    ptr_string += f\"{lf1[:4]}\\t\"\n",
    "    for k in range(0, i):\n",
    "        ptr_string += '\\t\\t'\n",
    "    for j in range(i, len(language_families)):\n",
    "        lf2 = language_families[j]\n",
    "        mean = round(np.mean(pairwise_ARI_families[f\"{lf1}-{lf2}\"]), 2)\n",
    "        std = round(np.std(pairwise_ARI_families[f\"{lf1}-{lf2}\"]), 2)\n",
    "        ptr_string += f\"{mean}({std})\\t\"\n",
    "    ptr_string += '\\n'\n",
    "print(ptr_string)\n",
    "\n",
    "# we have the following findings:\n",
    "\n",
    "# (1) the subnetworks of any language families are not enough resembling the ConceptNet\n",
    "\n",
    "# (2) ARIs between indo and otom and nucl are the lowest: 0.23, \n",
    "# which indicates that these two language families are less similar to indo\n",
    "\n",
    "# (3) ARI between atla and aust is the highest: 0.37,\n",
    "# which indicates that these two language families might be similar in terms of conceptualization\n",
    "\n",
    "# (4) ARIs between nucl with any other language families, expept aust,\n",
    "# are all lower than 0.3, which indicates the conceptualization of nucl\n",
    "# might be more or less unique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cf05e1",
   "metadata": {},
   "source": [
    "## Compare similarity of area specific networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f4f5dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_list = list(undirected_net.nodes)\n",
    "subgraph_dict_areas['base'] = undirected_net\n",
    "areas = list(subgraph_dict_areas.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06075e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_ARI_areas = {}\n",
    "\n",
    "for i in range(len(areas)):\n",
    "    a1 = areas[i]\n",
    "    net1 = subgraph_dict_areas[a1]\n",
    "    for j in range(i, len(areas)):\n",
    "        a2 = areas[j]\n",
    "        net2 = subgraph_dict_areas[a2]\n",
    "        # we randomly do 50 partitions for each language family\n",
    "        for i in range(50):\n",
    "            # make sure the random seed is the same when computing ARI between two families each time\n",
    "            seed = random.randint(0,114514)\n",
    "\n",
    "            # community partion of net1\n",
    "            partition_dict = community_louvain.best_partition(net1, resolution=0.1, random_state=seed)\n",
    "            partion_1 = [partition_dict[node] for node in nodes_list]\n",
    "\n",
    "            # community partion of net2\n",
    "            partition_dict = community_louvain.best_partition(net2, resolution=0.1, random_state=seed)\n",
    "            partion_2 = [partition_dict[node] for node in nodes_list]\n",
    "\n",
    "            if f\"{a1}-{a2}\" not in pairwise_ARI_areas:\n",
    "                pairwise_ARI_areas[f\"{a1}-{a2}\"] = [round(adjusted_rand_score(partion_1, partion_2), 2)]\n",
    "            else:\n",
    "                pairwise_ARI_areas[f\"{a1}-{a2}\"].append(round(adjusted_rand_score(partion_1, partion_2), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c68e4e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSA\t\tNA\t\tEurasia\t\tAfrica\t\tPapunesia\tbase\t\t\n",
      "SA\t1.0(0.0)\t0.58(0.03)\t0.51(0.03)\t0.56(0.03)\t0.56(0.02)\t0.56(0.03)\t\n",
      "NA\t\t\t1.0(0.0)\t0.55(0.03)\t0.61(0.04)\t0.62(0.03)\t0.62(0.02)\t\n",
      "Eurasia\t\t\t\t\t1.0(0.0)\t0.62(0.03)\t0.6(0.03)\t0.61(0.04)\t\n",
      "Africa\t\t\t\t\t\t\t1.0(0.0)\t0.76(0.04)\t0.78(0.03)\t\n",
      "Papunesia\t\t\t\t\t\t\t\t1.0(0.0)\t0.8(0.04)\t\n",
      "base\t\t\t\t\t\t\t\t\t\t\t1.0(0.0)\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ptr_string = [f\"\\t\"] + [f\"{a}\\t\\t\" for a, pl in subgraph_dict_areas.items()]\n",
    "for i in range(len(ptr_string)):\n",
    "    ptr_string[i] = ptr_string[i].replace('Papunesia\\t\\t', 'Papunesia\\t')\n",
    "    ptr_string[i] = ptr_string[i].replace('South America', 'SA')\n",
    "    ptr_string[i] = ptr_string[i].replace('North America', 'NA')\n",
    "\n",
    "ptr_string = ''.join(ptr_string) + '\\n'\n",
    "for i in range(len(areas)):\n",
    "    a1 = areas[i]\n",
    "    if a1 == 'South America':\n",
    "        ptr_string += f\"SA\\t\"\n",
    "    elif a1 == 'North America':\n",
    "        ptr_string += f\"NA\\t\"\n",
    "    else:\n",
    "        ptr_string += f\"{a1}\\t\"\n",
    "    for k in range(0, i):\n",
    "        ptr_string += '\\t\\t'\n",
    "    if a1 == 'Papunesia':\n",
    "        ptr_string = ptr_string[:-1]\n",
    "    for j in range(i, len(areas)):\n",
    "        a2 = areas[j]\n",
    "        mean = round(np.mean(pairwise_ARI_areas[f\"{a1}-{a2}\"]), 2)\n",
    "        std = round(np.std(pairwise_ARI_areas[f\"{a1}-{a2}\"]), 2)\n",
    "        ptr_string += f\"{mean}({std})\\t\"\n",
    "    ptr_string += '\\n'\n",
    "print(ptr_string)\n",
    "\n",
    "# we have the following findings:\n",
    "\n",
    "# (1) the subnetworks of Papunesia has the highest ARI with the ConceptNet, this indicates\n",
    "# that languages are the most DIVERGE in Papunesia, since ConceptNet considers associations in all languages\n",
    "\n",
    "# (2) although SA and NA are both in America, the conceptulization seems to be diverging:\n",
    "# with a mean of 0.66 ARI, relatively low compared to other pairs\n",
    "\n",
    "# (3) Africa and Papunesia have a high ARI, this might indicates that the languages contained \n",
    "# in these two areas can have very DIVERGE conceptualizations as their ARI with ConceptNet is also high,\n",
    "# with 0.85 and 0.88 respectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
